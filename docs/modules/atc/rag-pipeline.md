---
title: Pipeline RAG â€” Embeddings, BÃºsqueda y Chat IA
description: Arquitectura completa del pipeline RAG con HyDE, Pinecone y chat con herramientas
---

# ğŸ¤– Pipeline RAG â€” Retrieval-Augmented Generation

## Â¿QuÃ© es el RAG de ATC?

El sistema RAG (Retrieval-Augmented Generation) del mÃ³dulo ATC permite que el asistente responda preguntas con informaciÃ³n **verificada y actualizada** del restaurante, en lugar de depender del conocimiento genÃ©rico del LLM.

El pipeline tiene dos fases principales:

1. **IndexaciÃ³n** (offline) â€” Las entradas de KB se convierten en vectores y se almacenan en Pinecone
2. **RecuperaciÃ³n** (online) â€” Cuando el usuario hace una pregunta, se buscan los fragmentos mÃ¡s relevantes y se inyectan en el contexto del LLM

---

## ğŸ§± Componentes del Stack

| Componente | TecnologÃ­a | Rol |
|------------|-----------|-----|
| Embeddings | `text-embedding-3-small` via OpenRouter | Convertir texto a vectores de 1536 dimensiones |
| Vector DB | Pinecone v7 (serverless) | Almacenamiento y bÃºsqueda de similaridad coseno |
| LLM NormalizaciÃ³n | `gpt-4o-mini` via OpenRouter | Chunking y estructuraciÃ³n del conocimiento |
| LLM HyDE | `gemini-2.0-flash-lite` via OpenRouter | GeneraciÃ³n de respuesta hipotÃ©tica para mejorar retrieval |
| LLM Chat | Configurable (default `gpt-4o-mini`) | Respuesta final con tool calling |
| Framework | Vercel AI SDK v6 | Streaming y tool orchestration |

---

## ğŸ“¡ Fase de IndexaciÃ³n

### 1. GeneraciÃ³n de Embeddings

```typescript
// src/lib/embeddings.ts
const EMBEDDING_MODEL = "openai/text-embedding-3-small"

// Embedding individual (crear/actualizar una entrada)
export async function generateEmbedding(text: string): Promise<number[]>

// Embedding en lote de 100 (importaciÃ³n masiva)
export async function generateEmbeddingsBatch(texts: string[]): Promise<number[][]>
```

El texto que se embeddea no es solo el contenido, sino una composiciÃ³n que incluye el tÃ­tulo y la secciÃ³n:

```typescript
// src/lib/embeddings.ts
export function buildKBText(title: string, content: string, section?: string | null): string {
  return section ? `${title} â€” ${section}\n\n${content}` : `${title}\n\n${content}`
}
```

Incluir el tÃ­tulo en el texto embeddedo mejora significativamente el recall, ya que el vector captura tanto el tema (tÃ­tulo) como el contenido.

### 2. Metadatos del Vector

Cada vector en Pinecone almacena metadatos que permiten filtrar sin recurrir a la DB:

```typescript
interface KBVectorMetadata {
  title: string
  section?: string
  categoryId?: string
  source: string
  language?: string
  active: boolean    // â† Filtro principal en bÃºsquedas
}
```

### 3. SincronizaciÃ³n KB â†” Pinecone

Las server actions mantienen la sincronizaciÃ³n automÃ¡ticamente:

| OperaciÃ³n KB | AcciÃ³n Pinecone |
|-------------|----------------|
| Crear entrada | `upsertKnowledgeVector()` |
| Actualizar entrada | `upsertKnowledgeVector()` (misma ID) |
| Toggle activo/inactivo | `upsertKnowledgeVector()` (actualiza `active`) |
| Eliminar entrada | `deleteKnowledgeVectors([id])` |
| Borrar por source | `deleteVectorsBySource(source)` |

---

## ğŸ” Fase de RecuperaciÃ³n â€” HyDE Progresivo

La bÃºsqueda implementa una estrategia en dos etapas llamada **HyDE (Hypothetical Document Embeddings)**:

```
Pregunta del usuario
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Etapa 1: BÃºsqueda directa                 â”‚
â”‚  query â†’ embedding â†’ Pinecone (top-5)      â”‚
â”‚  threshold: 0.65                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              Â¿topScore < 0.70?
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             SÃ­                          No
              â”‚                           â”‚
              â–¼                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  Etapa 2: HyDE                      â”‚   â”‚
â”‚  query â†’ LLM hipotÃ©tico â†’          â”‚   â”‚
â”‚  respuesta hipotÃ©tica + query       â”‚   â”‚
â”‚  â†’ embedding â†’ Pinecone (top-5)    â”‚   â”‚
â”‚  threshold: 0.55                    â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                  â”‚                       â”‚
                  â–¼                       â–¼
        Fusionar resultados (dedup por ID, max score)
                  â”‚
                  â–¼
        Top-5 resultados finales
```

### Â¿Por quÃ© HyDE funciona?

El problema clÃ¡sico del RAG: una pregunta informal (`"Â¿tenÃ©is mesas para 8?"`) tiene un vector semÃ¡ntico diferente a una respuesta estructurada (`"Disponemos de salones privados para grupos de 8 a 20 personas..."`). HyDE soluciona esto generando primero una respuesta hipotÃ©tica que estarÃ¡ en el mismo espacio semÃ¡ntico que las entradas indexadas.

### Umbrales de Similaridad

```typescript
// src/app/api/atc/chat/tools.ts
const SCORE_THRESHOLD_DIRECT = 0.65  // MÃ­nimo para bÃºsqueda directa
const SCORE_THRESHOLD_HYDE = 0.55    // MÃ­nimo para bÃºsqueda HyDE (mÃ¡s permisivo)
const HYDE_TRIGGER = 0.70            // Si top score < 0.70 â†’ activar HyDE
```

Un score de 0.65 equivale a similaridad coseno del 65%, que en la prÃ¡ctica indica contenido "bastante relacionado". El threshold HyDE es mÃ¡s permisivo (0.55) porque la respuesta hipotÃ©tica ya actÃºa como filtro semÃ¡ntico.

### GeneraciÃ³n de Respuesta HipotÃ©tica

```typescript
// src/lib/embeddings.ts
export async function generateHyDEQuery(userQuery: string): Promise<string> {
  // Genera 2-3 frases como si el restaurante respondiera
  const hypothetical = await llm.complete("Eres un experto en restaurantes...")
  // Combina la respuesta hipotÃ©tica con la query original
  return `${hypothetical}\n\n${userQuery}`
}
```

La combinaciÃ³n `hipotÃ©tica + query original` captura ambas seÃ±ales semÃ¡nticas: la precisiÃ³n de la respuesta hipotÃ©tica y el lÃ©xico exacto del usuario.

---

## ğŸ’¬ Chat con Herramientas

### Arquitectura del Endpoint

```typescript
// src/app/api/atc/chat/route.ts
const result = streamText({
  model: getChatLanguageModel(),
  temperature: 0.1,           // Baja temperatura â†’ respuestas mÃ¡s precisas y reproducibles
  maxOutputTokens: 600,
  stopWhen: stepCountIs(5),   // MÃ¡ximo 5 iteraciones de tool calling
  system: SYSTEM_PROMPT,
  messages: history,          // Ãšltimas 6 mensajes de la conversaciÃ³n
  tools: { ... },
})
return result.toUIMessageStreamResponse()
```

El `stopWhen: stepCountIs(5)` previene bucles infinitos de tool calling. En la prÃ¡ctica, la mayorÃ­a de respuestas requieren 1-2 iteraciones.

### Herramientas Disponibles

#### `searchKnowledgeBase`
La herramienta principal. Implementa HyDE progresivo:

```typescript
inputSchema: z.object({
  query: z.string(),          // Pregunta en lenguaje natural
  categoryFilter: z.string().optional(),  // UUID de categorÃ­a (opcional)
})
// Devuelve: Array<{ id, title, section, content }>
```

El agente debe citar la fuente al usar esta herramienta: `[Fuente: nombre]`.

#### `lookupReservation`
Busca reservas activas (no CANCELLED ni NO_SHOW):

```typescript
inputSchema: z.object({
  guestName: z.string().optional(),  // BÃºsqueda parcial insensible a mayÃºsculas
  date: z.string().optional(),       // Formato YYYY-MM-DD
})
// Devuelve: mÃ¡ximo 5 reservas ordenadas por fecha/hora
```

#### `getActiveIncidents`
Incidencias activas (OPEN o IN_PROGRESS) + alertas meteorolÃ³gicas:

```typescript
inputSchema: z.object({})  // Sin parÃ¡metros â€” siempre devuelve el estado actual
// Devuelve: incidents[], weatherAlerts[], hasActiveIssues: boolean
```

#### `checkWaitingList`
Lista de espera sin notificar para una fecha:

```typescript
inputSchema: z.object({
  date: z.string(),  // Formato YYYY-MM-DD
})
// Devuelve: totalWaiting, primeras 8 entradas ordenadas por prioridad
```

### Sistema de Prompt

```
Eres el asistente ATC de Dreamland Restaurant.

REGLAS ESTRICTAS:
1. Para info de espacios/menÃºs/alÃ©rgenos/horarios â†’ usa SIEMPRE searchKnowledgeBase
2. Para reservas â†’ usa lookupReservation
3. NUNCA inventes datos. Si no hay resultados â†’ indÃ­calo claramente
4. Responde en espaÃ±ol, profesional, conciso y amable
5. Cita la fuente: [Fuente: nombre]
6. Si no puedes ayudar, indica quÃ© informaciÃ³n necesitarÃ­as
```

La temperatura de 0.1 y las reglas estrictas previenen alucinaciones.

---

## ğŸ“Š Trazabilidad de Consultas

Cada conversaciÃ³n queda registrada en base de datos al finalizar el streaming:

```typescript
// src/app/api/atc/chat/route.ts â€” onFinish callback
const query = await prisma.query.create({
  data: {
    guestInput: userQuery,
    categoryId: defaultCategory.id,
    channel: "WEB_RAG",
    status: scoreRef.value > 0 ? "RESOLVED" : "OPEN",  // SegÃºn si hubo match en KB
    confidenceScore: scoreRef.value,                      // Score coseno mÃ¡ximo obtenido
    resolvedBy: session.user?.id,
  },
})
await prisma.queryResolution.create({
  data: {
    queryId: query.id,
    responseText: text,       // Texto completo de la respuesta del LLM
    source: "AI",
  },
})
```

El `scoreRef` se pasa por referencia a la tool `searchKnowledgeBase` y captura el score mÃ¡ximo obtenido durante la sesiÃ³n. Si es > 0, la consulta se marca como `RESOLVED`.

---

## âš™ï¸ ConfiguraciÃ³n de Pinecone

### Setup del Ãndice

El cliente Pinecone usa lazy initialization con singleton:

```typescript
// src/lib/pinecone.ts
let _pinecone: Pinecone | null = null
let _index: Index | null = null

export function getPineconeIndex(): Index {
  if (!_index) {
    _index = getPineconeClient().index(process.env.PINECONE_INDEX_NAME || "dreamland-atc")
  }
  return _index
}
```

### ConfiguraciÃ³n del Ãndice (crear una vez en Pinecone Console)

```
Nombre:     dreamland-atc
Dimensiones: 1536              (text-embedding-3-small)
MÃ©trica:    cosine
Tipo:       serverless
Cloud:      AWS
Region:     us-east-1 (o el mÃ¡s cercano)
```

### Operaciones Pinecone v7

> **Importante**: La API de Pinecone v7 cambiÃ³. Usar siempre estos patrones:

```typescript
// âœ… Correcto â€” Pinecone v7
await index.upsert({ records: [{ id, values, metadata }] })
await index.deleteMany({ ids: ["id1", "id2"] })
await index.deleteMany({ filter: { source: { $eq: "gstock" } } })

// âŒ Incorrecto â€” API antigua
await index.upsert([{ id, values, metadata }])  // Array directo
await index.delete1({ ids: [...] })
```

El cast de metadata requiere double cast:
```typescript
metadata as unknown as Record<string, string | number | boolean | string[]>
```

---

## ğŸ­ Frontend â€” Chat UI

El chat usa **Vercel AI SDK v6** con patrones especÃ­ficos:

```typescript
// src/modules/atc/ui/chat.tsx (patrÃ³n)
import { useChat, DefaultChatTransport } from "ai"

const { messages, sendMessage, status } = useChat({
  transport: new DefaultChatTransport({
    api: "/api/atc/chat",
    body: { categoryId },
    fetch: customFetch,  // Para capturar headers de respuesta
  }),
})

// Enviar mensaje
sendMessage({ text: input }, { body: { categoryId } })

// Estado de carga
const isLoading = status === "submitted" || status === "streaming"

// Extraer texto de UIMessage.parts
const text = message.parts
  .filter(p => p.type === "text")
  .map(p => p.text)
  .join("")
```

### Renderizado de Markdown

Las respuestas del LLM se renderizan con `react-markdown` + `remark-gfm` para soporte completo de listas, negritas y tablas.

---

## ğŸ“ˆ MÃ©tricas y DiagnÃ³stico

### Score de Confianza

El `confidenceScore` guardado en `Query` permite analizar la calidad del RAG:

| Score | InterpretaciÃ³n |
|-------|---------------|
| 0.0 | Sin resultados en KB |
| 0.55 â€“ 0.64 | Resultado HyDE (baja confianza) |
| 0.65 â€“ 0.79 | Resultado directo (confianza media) |
| 0.80+ | Alta confianza â€” contenido muy relevante |

### Indicadores de Problemas

- Score sistemÃ¡ticamente < 0.55 â†’ La KB necesita mÃ¡s contenido sobre ese tema
- HyDE activÃ¡ndose frecuentemente â†’ Las queries de usuarios no coinciden semÃ¡nticamente con el contenido indexado (revisar redacciÃ³n de chunks)
- Muchas queries con `status: "OPEN"` â†’ Contenido no cubierto en KB

---

## ğŸ”§ Variables de Entorno

```bash
# Modelos (vÃ­a OpenRouter)
OPENROUTER_API_KEY=sk-or-...
AI_CHAT_MODEL=openai/gpt-4o-mini        # LLM principal para chat y normalizaciÃ³n
# HyDE usa HYDE_MODEL: google/gemini-2.0-flash-lite-001 (hardcoded en embeddings.ts)

# Pinecone
PINECONE_API_KEY=pcsk_...
PINECONE_INDEX_NAME=dreamland-atc
```

---

## ğŸ§ª Testing

El pipeline RAG tiene una suite completa de tests en `src/__tests__/atc/`:

| Archivo | Tests | Cubre |
|---------|-------|-------|
| `rag.test.ts` | 30 | Embeddings, HyDE, Pinecone ops, `searchSimilar()` |
| `rag-chat-tools.test.ts` | 24 | Tools del agente: `searchKnowledgeBase`, `lookupReservation`, `getActiveIncidents` |
| `rag-chat-tracing.test.ts` | 5 | Trazabilidad Query + QueryResolution vÃ­a `onFinish` |
| `rag-integration.test.ts` | 7 | Flujos de integraciÃ³n end-to-end con mocks de OpenAI + Pinecone |
| `knowledge-base-actions.test.ts` | 18 | CRUD KB, sync por fuente, bulk import con deduplicaciÃ³n |

```bash
npm run test:run                # Suite completa de unit tests (incluye ATC)
npm run test:e2e:rag           # Tests E2E contra APIs reales (requiere seed + ~5s espera)
```

Ver guÃ­a completa de testing: [GuÃ­a de Testing](/guides/testing)
