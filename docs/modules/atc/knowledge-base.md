---
title: Knowledge Base â€” GestiÃ³n de Conocimiento
description: ImportaciÃ³n, normalizaciÃ³n, deduplicaciÃ³n y gestiÃ³n de la base de conocimiento RAG
---

# ğŸ“š Knowledge Base

## Â¿QuÃ© es la Knowledge Base?

La **Knowledge Base (KB)** es el repositorio de informaciÃ³n estructurada que alimenta el motor RAG del asistente ATC. Cada entrada representa un fragmento de conocimiento (chunk) sobre el restaurante: horarios, menÃºs, alÃ©rgenos, espacios, polÃ­ticas, etc.

Cada chunk se almacena en **dos lugares**:
1. **PostgreSQL** (`knowledge_base` table) â€” texto completo, metadatos, hash de deduplicaciÃ³n
2. **Pinecone** (vector index) â€” embedding numÃ©rico para bÃºsqueda semÃ¡ntica

Ambos deben mantenerse sincronizados. Las server actions garantizan que cualquier operaciÃ³n (crear, actualizar, activar/desactivar, eliminar) se refleja en los dos sistemas.

---

## ğŸ“ Modelo de Datos

```prisma
model KnowledgeBase {
  id          String   @id @default(cuid())
  title       String                        // TÃ­tulo descriptivo del chunk
  content     String                        // Contenido del chunk (â‰¤ 400 tokens)
  contentHash String?                       // SHA-256 de title+content normalizado
  categoryId  String?                       // â†’ QueryCategory
  section     String?                       // SubsecciÃ³n del documento
  source      String   @default("manual")   // manual | staged | file | excel | gstock | n8n
  language    String   @default("es")       // es | en | de | fr | it | ru
  active      Boolean  @default(true)       // false = excluido de las bÃºsquedas
  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt

  @@unique([contentHash, source, language])  // DeduplicaciÃ³n
  @@index([categoryId])
  @@index([active])
  @@index([source])
  @@index([language])
}
```

### Campo `contentHash` â€” DeduplicaciÃ³n

El hash se calcula con SHA-256 sobre el contenido normalizado:

```typescript
// src/modules/atc/actions/knowledge-base.ts
function computeContentHash(title: string, content: string): string {
  const normalized = `${title.trim().toLowerCase()}||${content.trim().toLowerCase()}`
  return createHash("sha256").update(normalized).digest("hex")
}
```

El Ã­ndice Ãºnico `@@unique([contentHash, source, language])` garantiza que el mismo contenido no puede duplicarse dentro del mismo source e idioma. Esto permite que la misma informaciÃ³n exista en `source: "es"` y `source: "en"` (traducciones).

---

## ğŸ”„ Pipeline de ImportaciÃ³n

### Flujo completo

```
Archivo / Texto
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fase 1: Parsing                        â”‚
â”‚  /api/atc/knowledge-base/parse-file     â”‚
â”‚  â”€ Excel â†’ SheetJS â†’ ParsedSection[]   â”‚
â”‚  â”€ PDF   â†’ pdf-parse â†’ ParsedSection[] â”‚
â”‚  â”€ CSV   â†’ nativo â†’ ParsedSection[]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ ParsedSection[]
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fase 2: NormalizaciÃ³n IA               â”‚
â”‚  /api/atc/knowledge-base/normalize      â”‚
â”‚  â”€ GPT-4o-mini via OpenRouter           â”‚
â”‚  â”€ Divide en chunks â‰¤ 400 tokens       â”‚
â”‚  â”€ Sugiere tÃ­tulo, secciÃ³n, categorÃ­a  â”‚
â”‚  â”€ Detecta idioma                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ StagedEntry[]
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fase 3: RevisiÃ³n manual                â”‚
â”‚  UI: editar tÃ­tulo, secciÃ³n, categorÃ­a  â”‚
â”‚  Seleccionar/deseleccionar chunks       â”‚
â”‚  Toggle "Reemplazar importaciÃ³n ant."   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fase 4: PublicaciÃ³n                    â”‚
â”‚  publishStagedEntries() en lotes de 10  â”‚
â”‚  â”€ Genera embeddings (batch API)        â”‚
â”‚  â”€ Upsert en PostgreSQL con contentHash â”‚
â”‚  â”€ Upsert vectores en Pinecone          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Formatos de Archivo Soportados

### Excel (.xlsx / .xls)

- **LibrerÃ­a**: SheetJS (`xlsx`)
- **Estructura**: 1 secciÃ³n = 1 hoja del libro
- **Datos extraÃ­dos**: headers, rows como `Record<string, string>[]`
- **Formato al normalizar**: `- Header: valor` por fila
- **TamaÃ±o mÃ¡ximo**: 5MB

> **LimitaciÃ³n**: Las imÃ¡genes, grÃ¡ficos y drawing objects de Google Sheets/Excel no se extraen. Si el archivo contiene informaciÃ³n en imÃ¡genes (ej: horarios como captura de pantalla), debe pegarse manualmente en la pestaÃ±a "Texto".

### PDF (.pdf)

- **LibrerÃ­a**: `pdf-parse`
- **Estructura**:
  - PDF â‰¤ 10 pÃ¡ginas â†’ 1 secciÃ³n = documento completo
  - PDF > 10 pÃ¡ginas â†’ dividido en bloques de ~5.000 chars con corte en pÃ¡rrafo natural
- **Datos extraÃ­dos**: texto plano de todas las capas de texto
- **TamaÃ±o mÃ¡ximo**: 10MB

> **LimitaciÃ³n**: Solo extrae texto de capas PDF. PDFs escaneados (imÃ¡genes sin OCR) producirÃ¡n secciones vacÃ­as.

### CSV (.csv)

- **LibrerÃ­a**: Parser nativo (sin dependencia externa)
- **Estructura**: 1 secciÃ³n = archivo completo
- **Datos extraÃ­dos**: headers de la primera fila, rows como tabla
- **Soporte**: valores con comillas, comas escapadas
- **TamaÃ±o mÃ¡ximo**: 5MB

---

## ğŸ§  NormalizaciÃ³n con IA

### API: `POST /api/atc/knowledge-base/normalize`

El endpoint recibe texto libre (o texto formateado desde un archivo) y lo estructura en chunks:

```typescript
// Request
{ text: string, source?: "excel" | "file" | undefined }

// Response
{
  entries: Array<{
    title: string
    section: string
    content: string
    categorySuggestion: string   // "espacios" | "menus" | "horarios" | ...
    language: string             // "es" | "en" | ...
    tokenCount: number
  }>
}
```

### System Prompt

El LLM recibe instrucciones para:
1. Dividir en chunks de **mÃ¡ximo 400 tokens**
2. Generar un tÃ­tulo claro y descriptivo
3. Identificar la secciÃ³n temÃ¡tica
4. Sugerir una de las 11 categorÃ­as predefinidas
5. Generar un chunk separado por idioma si detecta contenido multilingÃ¼e
6. Anonimizar datos personales (emails â†’ `[EMAIL]`, telÃ©fonos â†’ `[TELÃ‰FONO]`)

Cuando `source: "file"`, se aÃ±ade contexto extra al prompt explicando que el contenido puede venir de Excel (filas formateadas) o PDF (texto continuo).

### ResoluciÃ³n de CategorÃ­as

La sugerencia del LLM es un string en espaÃ±ol (`"reservas"`, `"menus"`) que se mapea a un UUID de base de datos:

```typescript
// src/modules/atc/ui/knowledge-base/kb-import-panel.tsx
const SUGGESTION_TO_CODE: Record<string, string> = {
  espacios: "SPACES",      alergenos: "ALLERGENS",
  accesibilidad: "ACCESSIBILITY",  horarios: "SCHEDULES",
  menus: "MENUS",          politicas: "POLICIES",
  general: "GENERAL",      reservas: "RESERVATIONS",
  pagos: "PAYMENTS",       eventos: "EVENTS",
  incidencias: "INCIDENTS",
}

function resolveCategoryId(suggestion: string, categories: QueryCategory[]): string | undefined {
  const code = SUGGESTION_TO_CODE[suggestion.toLowerCase().trim()]
  return categories.find(c => c.code === code)?.id
}
```

---

## ğŸ”’ DeduplicaciÃ³n

### Funcionamiento

Al importar en lote (`bulkImportKnowledgeBaseEntries`), el sistema:

1. Calcula el `contentHash` para cada entrada
2. Consulta la DB por hashes existentes con el mismo `source` e idioma
3. Filtra las entradas ya existentes â†’ solo procesa las nuevas
4. Genera embeddings **solo** para las entradas nuevas (ahorro de API calls)
5. Persiste en DB con hash y sube vectores a Pinecone

```typescript
// DeduplicaciÃ³n en bulk import
const existingSet = new Set(existing.map(e => `${e.contentHash}|${e.source}|${e.language}`))
indicesToProcess = indicesToProcess.filter(i => {
  const key = `${hashes[i]}|${entries[i].source ?? "n8n"}|${entries[i].language ?? "es"}`
  return !existingSet.has(key)
})
```

### Modo Reemplazo

El toggle "Reemplazar importaciÃ³n anterior" en la UI elimina todas las entradas del mismo `source` antes de publicar, actualizando asÃ­ el contenido:

```typescript
// handlePublish en kb-import-panel.tsx
if (replaceMode) {
  await deleteKnowledgeBaseBySource(source)  // Borra DB + Pinecone
}
await publishStagedEntries(batch, source)
```

---

## ğŸ—‘ï¸ Borrado y Limpieza

### Borrado individual
Elimina el vector de Pinecone antes de borrar el registro en DB para evitar inconsistencias:

```typescript
export async function deleteKnowledgeBaseEntry(id: string) {
  await deleteKnowledgeVectors([id])  // Pinecone primero
  await prisma.knowledgeBase.delete({ where: { id } })
}
```

### Borrado masivo por source
Disponible desde la tabla KB via el dropdown "Borrado masivo":

```typescript
export async function deleteKnowledgeBaseBySource(source: string) {
  await deleteVectorsBySource(source)           // Filtro en Pinecone por metadata.source
  await prisma.knowledgeBase.deleteMany({ where: { source } })
}
```

> **Nota**: Los sources `"manual"` no aparecen en el dropdown de borrado masivo para prevenir eliminaciÃ³n accidental.

---

## âš¡ PublicaciÃ³n por Lotes

Para evitar timeouts en server actions con muchas entradas, la publicaciÃ³n se procesa en **lotes de 10**:

```typescript
// kb-import-panel.tsx â€” handlePublish
const BATCH_SIZE = 10
for (let i = 0; i < toPublish.length; i += BATCH_SIZE) {
  const batch = toPublish.slice(i, i + BATCH_SIZE)
  await publishStagedEntries(batch.map(e => ({ ... })), source)
  setProgress({ current: Math.min(i + BATCH_SIZE, toPublish.length), total: toPublish.length })
}
```

Cada lote genera embeddings en batch (API call Ãºnico para 10 entradas), hace upsert en DB y en Pinecone.

---

## ğŸ”Œ IntegraciÃ³n n8n / Webhook

### Sync GStock: `POST /api/atc/knowledge-base/sync-gstock`

Endpoint protegido con `x-n8n-secret` para actualizaciÃ³n completa del source `"gstock"`:

```typescript
// AutenticaciÃ³n: header x-n8n-secret debe coincidir con N8N_WEBHOOK_SECRET
// Body: { entries: BulkKBEntry[] }
// OperaciÃ³n: DELETE all gstock â†’ INSERT nuevos â†’ Embed â†’ Upsert Pinecone
```

Este endpoint implementa una **sincronizaciÃ³n full-replace**: borra todos los vectores y entradas anteriores del source y los reemplaza. Garantiza que la informaciÃ³n siempre estÃ© actualizada.

### Import genÃ©rico n8n: `POST /api/atc/knowledge-base/import`

Para importaciones incrementales con deduplicaciÃ³n automÃ¡tica (sin borrar datos previos).

---

## ğŸ”§ Backfill de contentHash

Si hay entradas existentes sin `contentHash`, ejecutar el script de backfill:

```bash
npx tsx scripts/backfill-content-hash.ts
```

El script usa el mismo `computeContentHash()` que las server actions y actualiza en masa. Las entradas duplicadas (mismo hash) se reportan pero no se eliminan automÃ¡ticamente.

---

## ğŸ“Š Variables de Entorno Necesarias

```bash
DATABASE_URL=           # PostgreSQL connection string
DIRECT_URL=             # Directa para migraciones (Supabase)
OPENROUTER_API_KEY=     # Para normalizaciÃ³n con GPT-4o-mini
AI_CHAT_MODEL=          # Modelo a usar (default: openai/gpt-4o-mini)
PINECONE_API_KEY=       # API key de Pinecone
PINECONE_INDEX_NAME=    # Nombre del Ã­ndice (default: dreamland-atc)
N8N_WEBHOOK_SECRET=     # Secreto para autenticar webhooks n8n
```
